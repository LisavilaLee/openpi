好的，我将分步骤为您详细讲解如何使用 `uv` 环境复现 $\pi_0$ / $\pi_{0.5}$ 在 LIBERO benchmark 上的结果，包括环境配置、数据准备、训练复现以及仿真验证（可视化）。

### 项目分析与定制化指南

在开始之前，先回答您关于代码定制化的地方：

1.  **Checkpoints 保存位置**:
    - 默认位于项目根目录下的 `./checkpoints`。
    - **修改位置**: [`openpi/src/openpi/training/config.py`](openpi/src/openpi/training/config.py) 中的 `TrainConfig` 类，修改 `checkpoint_base_dir` 属性。
    - **权重加载**: 预训练权重默认从 Google Cloud Storage (`gs://`) 下载并缓存到 `~/.cache/openpi` (Linux) 或由 `OPENPI_DATA_HOME` 环境变量控制。

2.  **数据集位置**:
    - 默认使用 Hugging Face 上的 LeRobot 数据集格式。
    - **修改位置**: [`openpi/src/openpi/training/config.py`](openpi/src/openpi/training/config.py) 中的 `LeRobotLiberoDataConfig` 类。如果您有自定义转换的数据集，需修改 `repo_id` 为您的本地路径或 HF ID。
    - **原始数据转换**: [`examples/libero/convert_libero_data_to_lerobot.py`](examples/libero/convert_libero_data_to_lerobot.py) 脚本用于将原始 LIBERO 数据转换为训练所需的格式。

3.  **视频输出位置**:
    - 仿真验证时生成的视频默认保存在 `data/libero/videos`。
    - **修改位置**: [`examples/libero/main.py`](examples/libero/main.py) 中的 `video_out_path` 参数。

---

### 第一步：环境配置 (使用 uv)

由于 LIBERO 评测代码依赖较多（如 MuJoCo），且与模型训练部分可能存在依赖冲突，建议**模型训练/服务**与**LIBERO 仿真评测**使用两个独立的环境（或严格遵循以下顺序）。

#### 1. 安装 uv
如果您尚未安装 `uv`，请运行：
```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
source $HOME/.cargo/env
```

#### 2. 配置主环境 (用于训练和启动模型服务)
这是运行 $\pi_0$ 模型的主环境。

```bash
# 1. 克隆项目 (记得带子模块)
git clone --recurse-submodules git@github.com:Physical-Intelligence/openpi.git
cd openpi

# 2. 初始化环境并安装依赖
# 注意：GIT_LFS_SKIP_SMUDGE=1 是为了加速拉取 LeRobot 依赖
GIT_LFS_SKIP_SMUDGE=1 uv sync
GIT_LFS_SKIP_SMUDGE=1 uv pip install -e .

# 3. 激活环境
source .venv/bin/activate
```

#### 3. 配置 LIBERO 评测环境 (用于运行仿真器)
LIBERO 仿真器作为一个独立的客户端运行，建议单独配置以避免冲突。我们将此环境创建在 `examples/libero/.venv`。

```bash
# 新开一个终端窗口，或者先 deactivate 当前环境
cd examples/libero

# 创建 python 3.8 环境 (LIBERO 推荐)
uv venv --python 3.8 .venv
source .venv/bin/activate

# 安装依赖 (注意 MuJoCo 需要的 CUDA 依赖)
uv pip sync requirements.txt ../../third_party/libero/requirements.txt --extra-index-url https://download.pytorch.org/whl/cu113 --index-strategy=unsafe-best-match

# 安装 openpi-client 和 libero
uv pip install -e ../../packages/openpi-client
uv pip install -e ../../third_party/libero

# 设置 PYTHONPATH
export PYTHONPATH=$PYTHONPATH:$PWD/../../third_party/libero
```

---

### 第二步：准备数据集

如果您想复现训练，需要先准备 LIBERO 数据集。

1.  **下载原始数据**:
   您需要下载 `modified_libero_rlds` 数据集。由于项目未提供自动下载脚本，您可以使用 `huggingface-cli` 下载：
   ```bash
   # 在主环境中运行
   pip install huggingface_hub
   huggingface-cli download openvla/modified_libero_rlds --repo-type dataset --local-dir ./data/libero_raw
   ```

2.  **转换数据格式**:
   使用项目提供的脚本将数据转换为 LeRobot 格式。
   ```bash
   # 在主环境 (openpi根目录) 中运行
   # 确保安装了 tensorflow_datasets (转换脚本需要)
   uv pip install tensorflow tensorflow_datasets
   
   # 运行转换
   uv run examples/libero/convert_libero_data_to_lerobot.py --data_dir ./data/libero_raw
   ```
   转换后的数据默认会保存在 `~/.cache/huggingface/lerobot/` 下，或者由环境变量 `HF_LEROBOT_HOME` 指定。

---

### 第三步：复现训练 (Training)

使用 `pi05_libero` 配置复现 $\pi_{0.5}$ 在 LIBERO 上的微调。

```bash
# 在主环境 (openpi根目录) 中运行

# 1. 计算归一化统计量 (这一步是必须的)
uv run scripts/compute_norm_stats.py --config-name pi05_libero

# 2. 开始训练
# exp-name: 实验名称，用于区分不同的训练跑
# overwrite: 如果实验目录存在则覆盖
# XLA_PYTHON_CLIENT_MEM_FRACTION=0.9: 让 JAX 使用 90% GPU 显存
XLA_PYTHON_CLIENT_MEM_FRACTION=0.9 uv run scripts/train.py pi05_libero --exp-name=reproduce_libero --overwrite
```
训练完成后，权重会保存在 `checkpoints/pi05_libero/reproduce_libero/` 目录下。

---

### 第四步：仿真验证与可视化 (Evaluation)

这一步最关键，我们将**同时运行**两个进程：一个是**策略服务器 (Policy Server)**，一个是**仿真客户端 (Libero Client)**。客户端将状态发送给服务器，服务器返回动作，客户端在仿真器中执行并录制视频。

#### 1. 启动策略服务器 (Terminal 1 - 主环境)

您既可以加载官方提供的预训练权重，也可以加载您自己训练出来的权重。

**选项 A: 加载官方预训练权重**
```bash
# 在 openpi 根目录下，主环境中
# config=pi05_libero 会自动下载官方权重
uv run scripts/serve_policy.py policy:checkpoint --policy.config=pi05_libero
```

**选项 B: 加载您自己训练的权重**
假设您训练了 20000 步，权重路径为 `checkpoints/pi05_libero/reproduce_libero/20000`。
```bash
# 在 openpi 根目录下，主环境中
uv run scripts/serve_policy.py policy:checkpoint --policy.config=pi05_libero --policy.dir=checkpoints/pi05_libero/reproduce_libero/20000
```
*注：服务器启动后会监听 8000 端口，等待客户端连接。*

#### 2. 启动仿真客户端 (Terminal 2 - LIBERO 环境)

这个脚本会连接服务器，运行 LIBERO 任务，并保存视频。

```bash
# 切换到 openpi 根目录
# 激活之前配置的 LIBERO 环境
source examples/libero/.venv/bin/activate
export PYTHONPATH=$PYTHONPATH:$PWD/third_party/libero

# 运行评测
# 如果遇到 EGL 错误 (通常发生在非 headless 服务器上)，添加 MUJOCO_GL=glx
# video_out_path 指定视频保存路径
MUJOCO_GL=glx python examples/libero/main.py \
    --host localhost \
    --port 8000 \
    --task-suite-name libero_10 \
    --video-out-path data/libero/videos
```

- `--task-suite-name`: 可以选择 `libero_spatial`, `libero_object`, `libero_goal`, `libero_10`, `libero_90`。
- `--video-out-path`: 运行结束后，您可以在该目录下找到 `.mp4` 格式的抓取效果视频。

### 总结
1.  **环境**: 推荐分离 `openpi` 主环境和 `libero` 评测环境。
2.  **数据**: 下载 raw rlds 数据 -> 运行 [`examples/libero/convert_libero_data_to_lerobot.py`](examples/libero/convert_libero_data_to_lerobot.py) 脚本。
3.  **训练**: [`openpi/scripts/train.py`](openpi/scripts/train.py) `pi05_libero`。
4.  **验证**:
    - Term 1: [`openpi/scripts/serve_policy.py`](openpi/scripts/serve_policy.py) (加载模型)。
    - Term 2: [`examples/libero/main.py`](examples/libero/main.py) (运行仿真并录像)。
5.  **查看结果**: 检查 `data/libero/videos` 目录下的 MP4 文件。
